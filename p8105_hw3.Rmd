---
title: "Homework3"
author: "Avery Wang"
date: 2024-10-12
output: github_document
---

## Problem 1
Load the package and set up r
```{r}
library(tidyverse)
library(ggridges)
library(p8105.datasets)
library(patchwork)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```

Load dataset
```{r}
data("ny_noaa")
```


This dataset contains `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. It includes variables id, date, prcp (precipitation), snow (snowfall), snwd (snow depth), tmax and tmin (the max and min of temperature). It is also observed that the missing data frequently appears in the dataset especially in column tmax and tmin, which are important columns. So the first step of the data cleaning is to drop the rows with the missing values.

Then the year, month and day values are extracted using `lubridate`. And since the current prcp, tmax and tmax are tenths of mm and degrees C, these columns should be divided by 10. For tmax and tmin, the type should be converted to numeric first to do the calculation.


```{r}
cleaned_noaa= ny_noaa |> drop_na() |>
  #seperate year,month and date
  mutate(year=lubridate::year(date), # extract year
         month=lubridate::month(date), #extract month
         day = lubridate::day(date),#extract day
         prcp = prcp / 10,
         tmax=as.numeric(tmax), #convert to numeric to be divided by 10
         tmin=as.numeric(tmin),
         tmax = tmax / 10,
         tmin = tmin / 10) |>
  #reorder the dataset
  select(id,date,year,month,day,everything())
```


* For snowfall, what are the most commonly observed values?
```{r}
snow=cleaned_noaa |> group_by(snow) |>
  summarize(count=n()) |>
  arrange(desc(count)) |> head(5)
snow
```


From the observation the most commonly observed value for snowfall is `r snow$snow[1]`. It probably because NY does not snow that often. And the other most commonly observed values are 25,13,51,5 are also reasonable because they are the moderate snowfalls that could occur during winter seasons, not too heavy but noticeable.

* Make a two-panel plot showing the average max temperature in January and in July in each station across years.


```{r}
filtered_data = cleaned_noaa |>
  # filter with month Jan and July
  filter(month == 1 | month == 7) |>
  mutate(month = as.character(month)) |>
  #group by id, month and year
  group_by(id, month, year) |>
  # summarize with the average max temperature by group
  summarize(avg_max = mean(tmax, na.rm = TRUE))
```


```{r}
filtered_data |> ggplot(aes(x = year, y = avg_max,color = id)) +
  #use scatterplot
  geom_point(show.legend = F,alpha = .5) + geom_path(show.legend = F) +
  facet_grid(~month, labeller = as_labeller(c(`1` = "January", `7` = "July"))) +   
  labs(
    #give title
    title = "Average Max Temperature in January and July by Station Across Years",
    #show x and y axis 
    x = "Year",
    y = "Average Max Temperature (Â°C)"
  ) +theme_minimal()
```

The average max temperature is much higher in July than in January which is normal for the summer season and the winter season in NY. And it seems that the average max temperature falls in similar range for different years. There are some outliers in the plot such as the observation of extremely low temperature in January, 1982. It also shows the outlier during January in 1996. And in the year 1987 and month July, it is also observed a cold station, with is much lower than the temperatures in the other stations

* Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r}
hex= cleaned_noaa|>ggplot(aes(x = tmax,y=tmin)) + geom_hex()

ridge = 
  cleaned_noaa |>
  filter(snow < 100 & snow > 0) |>
  ggplot(aes(x = snow, y = as.factor(year))) + 
  geom_density_ridges()+
  labs(y = "Year")

hex + ridge
```

From the hex plot it is seen that the majority of the data locates near the center of the plot, and in most cases tmax is larger than tmin, but in some cases tmin is bigger. And for the ridge plot, the peak at 0 snowfall appears consistent, and there are also varying smaller peaks across other snowfall amounts like 20, 40, and beyond, reflecting how snow events of different sizes occurred across the years.

## Problem 2
load the data and drop the missing value for the demographic dataset
```{r}
#drop the missing demographic dat
demographic_data=read_csv(file = "./data/nhanes_covar.csv", na = c(".", "NA", ""),skip=4) |>
  janitor::clean_names() |> drop_na()

accelerometer_data = read_csv(file="./data/nhanes_accel.csv", na = c(".", "NA", "")) |>
  janitor::clean_names()

```

Merged the data set using left join on the same variable `seqn` value.
```{r}
merged_data=left_join(demographic_data,accelerometer_data, by="seqn")
```


clean the joined dataset by filtering out age under 21, encode the sex and education case
```{r}
cleaned_merged_data= merged_data|> filter(age>=21) |>
  mutate(
    sex = factor(sex, levels = c(1, 2), labels = c("male", "female")),
    education = factor(education, levels = c(1, 2, 3), 
                       labels = c("Less than high school", 
                                  "High school equivalent", "More than high school"))
  ) 
```

The new datasets now include `r nrow(cleaned_merged_data)` rows and `r ncol(cleaned_merged_data)` columns, with all the variables from the demographic and accelerometer data such as `seqn`, `sex`, `age`, `bmi`, `education` and the `*MIMS` values for each minutes of a 24-hour day.

* Provide a user-friendly table for the number of men and women in each education category

```{r}
#group by education and size
table_education_sex= cleaned_merged_data |> 
  group_by( education,sex) |>
  summarize(count=n()) |> 
  #pivot_wider to make more user friendly
  pivot_wider(names_from = sex,values_from = count) |>
  #create the table 
  knitr::kable(align = 'c')
 
table_education_sex

```

```{r}
#plot the age distribution using boxplot 
cleaned_merged_data |> ggplot(aes(x=education,y=age,fill=sex))+geom_boxplot(alpha = .5)+
  labs(
    title = "Age distribution for different education category",
    x = "education",
    y = "age",
    color = "sex",
   )  +theme_minimal()
```

From the table and the plot , it shows that for the category "less than high school" and "More than high school ", women and men shows about the similar count. And there are more males than females with education levels "equivalent to high school." 

* Aggregate across minutes to create a total activity variable for each participant

```{r}
#used for slicing
end_col=ncol(cleaned_merged_data)
aggregated_data=cleaned_merged_data |> 
  #aggregate by selecting all columns starting with min and name it total
  mutate(total=rowSums(cleaned_merged_data[6:end_col]))|>
  select(seqn:education,total)
```


* Plot these total activities (y-axis) against age (x-axis) and compare with sex for each education level

```{r}
aggregated_data |>ggplot(aes(x=age,y=total,color=sex))+
  geom_point(alpha = .5) +
  #include smooth line
  geom_smooth(se = FALSE) +
  #seperate panels for each education level
  facet_grid(. ~ education)+
  #add title 
  labs(title = "Total Activity by Different Age, Sex and Education Level",
       x = "age ",
       y = "total activity ") 
```

It is observed from the plot that for both male and female, the age gets older, the total activity time gets smaller. No matter which education level female and male groups have, it all shows the highest total activity before the age of 60, probably because people before that age maintain better health condition and energy. For the education level less than high school, male group tends to have more total activity, and the peak total activity falls between age 40 to 60 for both groups. And for the other two categories, female group tends to have more total activity. 

* Make a three-panel plot that shows the 24-hour activity time courses for each education level and use color to indicate sex

```{r}
activity_data=cleaned_merged_data |> pivot_longer(cols = min1: min1440,
                                                  names_to="minute",
                                                  values_to = "activity"
                                                  )
```

```{r}
knitr::opts_chunk$set(
  fig.width = 10,
  fig.asp = .6,
  out.width = "100%"
)

activity_data |> 
  mutate(minute_num = as.numeric(sub("min", "", minute)),  # Extract numeric minute value
          # Convert to hours (1 to 24)
         hour = minute_num %/% 60 + 1) |> 
  ggplot(aes(x = hour, y = activity, color = sex)) + 
  geom_point(alpha = .5) + 
  geom_smooth(se = FALSE) + 
  facet_grid(. ~ education) + 
  scale_x_continuous(
    breaks = 1:24,  # Set breaks at each hour
    labels = as.character(1:24)  # Label breaks as 1 through 24
  ) +
  labs(
    title = "24-hour Activity Time Course by Education Level and Sex",
    x = "Hour of Day",
    y = "Activity Level"
  )+theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Across all education levels, activity tends to peak in the morning (around 7am to 1 pm) and around 6 to 11 pm for both groups. For the group with education level more than high school, male group shows greater variability, with more pronounced peaks in activity and more outliers, such as some individuals like to have high activity level at about 8 to 10 pm. Male and female group generally shows similar activities for all three education levels.


## Problem 3

Load the datasets, and reorder with year and month as the first two columns. Standardize the columns with `janitor::clean_names()`. Add year and month for each dataset.

```{r}
Jan_2020=read_csv(file = "./data/citibike/Jan_2020_Citi.csv", na = c(".", "NA", "")) |>
  janitor::clean_names() |> mutate(year=2020,month="Janurary") |>
  select(year,month,everything())

Jan_2024=read_csv(file = "./data/citibike/Jan_2024_Citi.csv", na = c(".", "NA", "")) |>
  janitor::clean_names() |>mutate(year=2024,month="Janurary") |>
  select(year,month,everything())

July_2020=read_csv(file = "./data/citibike/July_2020_Citi.csv", na = c(".", "NA", "")) |>
  janitor::clean_names() |> mutate(year=2020,month="July") |> 
  select(year,month,everything())

July_2024=read_csv(file = "./data/citibike/July_2024_Citi.csv", na = c(".", "NA", "")) |>
  janitor::clean_names() |> mutate(year=2024,month="July") |>
  select(year,month,everything())
```

Combine these four datasets using `bind_rows`

```{r}
citi_data=bind_rows(Jan_2020, July_2020, Jan_2024,July_2024) |>drop_na()
```

The new dataset `citi_data` now has `r nrow(citi_data)` rows and `r ncol(citi_data)` columns with variables including `year` `month` `ride_id` `rideable_type` `weekdays` `duration` and information about stations and membership.



* Produce a reader-friendly table showing the total number of rides in each combination of year and month separating casual riders and Citi Bike members.

```{r}
ride_table= citi_data |>
  group_by(year,month,member_casual) |> 
  #summarize with the count of each group
  summarize(count=n()) |>
  #use pivot wider for user friendly 
  pivot_wider(
    names_from = member_casual,
    values_from = count,
    values_fill = 0
  )|>
 #knit the table 
  knitr::kable(
    # add the name for each column
    col.names = c("Year", "Month",  "Casual Riders", "Citi Bike Member"),
    align = 'c')
ride_table
```

The table shows the total number of rides taken by casual riders and Citi Bike members across two years, 2020 and 2024, in both January and July. Citi bike attract more casual and membership users from year 2020 to 2024. And people are more likely to use citi-bike during the summer seasons (July) compare with winter seasons (January). It follows the common sense as better weather for outdoor activities attract more people to use the Citi bike. 

* Plot the top fove most popular starting station for July 2024

```{r}
popular_start_station= citi_data|> filter(year==2024, month=="July") |>
  group_by(start_station_name) |> summarize(count=n()) |> 
  arrange(desc(count))|>
  head(5)|>
  knitr::kable(
    col.names = c("start station name", "Number of rides"),
    align = 'c') 
popular_start_station

```


From the table, it shows that Pier 61 attracts the most users probably because it is settled next to the long walkway, so people choose Citi bikes for exercise or enjoying a leisurely ride alongside the hudson river. And the second most popular station is the University Pl & E 14 St, located near universities, bussiness centers and a lot of residential areas, making a key spot for students, employees, and residents alike  So it is observed that stations located near business centers and areas with high foot traffic tend to be more popular.

* Make a plot to investigate the effects of day of the week, month, and year on median ride duration

```{r}
effect_median = citi_data |> 
  #make year factor type
  mutate(year=as.factor(year),
         #reorder with the level 
         weekdays = factor(weekdays, 
                           levels = c("Monday", "Tuesday", "Wednesday", 
                                      "Thursday", "Friday", "Saturday", 
                                      "Sunday"))) |>
  group_by(weekdays,month,year) |>
  #summarize the median of the ride duration
  summarize(median_duration=median(duration))
effect_median 
```

```{r}

effect_median |>
  arrange(year, month, weekdays) |> 
  ggplot(aes(x = weekdays, y = median_duration, 
             color = year, group = interaction(year, month))) +  # Group by both year and month
  geom_line() + 
  geom_point() + 
  facet_grid(year ~ month) +  # Facet by both year and month
  labs(title="Effects of day of the week, month, and year on median ride duration")+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

It is observed from the plot that the median ride in 2020 is larger than that in 2024. And for these two years, July shows the higher median duration. During January, the median duration does not fluctuate much across the whole week. But in July, it is obvious that the median of duration increases from Friday to Saturday.

*Make a figure that shows the impact of month, membership status, and bike type on the distribution of ride duration

Filter out the date in 2024 first
```{r}
analysis_2024= citi_data |> filter(year==2024) 
```


```{r}
 analysis_2024 |> 
  ggplot(aes(x = duration, fill = member_casual)) +
  geom_density(alpha = 0.5) +
  facet_grid(rideable_type ~ month) +  # Facet by bike type and month
  labs(
    title = "Density of Ride Duration by Month, Membership, and Bike Type",
    x = "Ride Duration (minutes)",
    fill = "Membership"
  ) +
  theme_minimal()

```

For both casual riders and members, the majority of rides are concentrated within the 0-50 minute range, with few rides exceeding 50 minutes. Members (yellow) seem to have a slightly higher peak than casual riders (purple), indicating that members tend to take short rides more frequently. And it seems that when taking the classic_bike, casual riders have a broader distribution, indicating that riders without the membership are more likely to take the classic bike for a longer time duration. The pattern of ride duration is fairly consistent across both January and July